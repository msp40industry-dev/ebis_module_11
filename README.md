# ü§ñ Chatbot Python con RAG - POC Academia de Programaci√≥n

Asistente conversacional especializado en Python que utiliza Retrieval-Augmented Generation (RAG) para responder preguntas sobre programaci√≥n. El sistema combina recuperaci√≥n sem√°ntica de documentaci√≥n indexada con generaci√≥n de respuestas contextualizadas mediante GPT-4o.

## üìã Tabla de Contenidos

- [Caracter√≠sticas](#caracter√≠sticas)
- [Stack T√©cnico](#stack-t√©cnico)
- [Arquitectura](#arquitectura)
- [Requisitos Previos](#requisitos-previos)
- [Instalaci√≥n](#instalaci√≥n)
- [Configuraci√≥n](#configuraci√≥n)
- [Uso](#uso)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Troubleshooting](#troubleshooting)

## ‚ú® Caracter√≠sticas

- **üí¨ Chat con historial completo**: El modelo mantiene el contexto de toda la conversaci√≥n
- **üîç RAG sobre FAQ de Python**: Recupera respuestas relevantes desde una base vectorial antes de generar la respuesta
- **üñºÔ∏è Soporte de im√°genes**: Adjunta capturas de c√≥digo PNG para an√°lisis visual
- **üéôÔ∏è Voz a texto**: Transcripci√≥n local con Vosk (sin enviar audio a la nube)
- **üìä Monitorizaci√≥n con MLflow**: Tracking completo de par√°metros, m√©tricas y trazas por ejecuci√≥n

## üõ†Ô∏è Stack T√©cnico

### Backend
- **FastAPI**: API REST
- **OpenAI GPT-4o**: Generaci√≥n de respuestas
- **Pinecone Local**: Base de datos vectorial (v√≠a Docker)
- **OpenAI text-embedding-3-small**: Generaci√≥n de embeddings
- **Vosk**: Speech-to-text en espa√±ol (modelo `vosk-model-small-es-0.42`)
- **MLflow**: Tracking de experimentos

### Frontend
- **Streamlit**: Interfaz de usuario personalizada

### Infraestructura
- **Docker**: Para ejecutar Pinecone Local
- **Python 3.13**: Entorno de ejecuci√≥n

## üèóÔ∏è Arquitectura

```
‚îú‚îÄ‚îÄ backend.py                    # Aplicaci√≥n FastAPI principal
‚îú‚îÄ‚îÄ frontend.py                   # Interfaz Streamlit
‚îú‚îÄ‚îÄ indexing_code.py             # Script para indexar documentos en Pinecone
‚îú‚îÄ‚îÄ faq_pairs.json               # Documentos FAQ sobre Python
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ chat_with_history.py     # Endpoint de chat con RAG
‚îÇ   ‚îî‚îÄ‚îÄ transcribe.py            # Endpoint de transcripci√≥n de voz
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îú‚îÄ‚îÄ rag_generation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_generation.py    # L√≥gica RAG (retrieval + generation)
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îî‚îÄ‚îÄ mlflow_setup.py      # Configuraci√≥n MLflow
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ vosk-model-small-es-0.42/  # Modelo de transcripci√≥n de voz
```

## üì¶ Requisitos Previos

Aseg√∫rate de tener instalado:

- **Python 3.13+**
- **Docker Desktop** (para Pinecone Local)
- **Una API Key de OpenAI** ([obtener aqu√≠](https://platform.openai.com/api-keys))

## üöÄ Instalaci√≥n

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/chatbot-python-rag.git
cd chatbot-python-rag
```

### 2. Crear entorno virtual

```bash
python -m venv poc_venv
source poc_venv/bin/activate  # En Windows: poc_venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Descargar el modelo Vosk

```bash
# macOS/Linux
curl -L -o vosk-model-small-es-0.42.zip https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip
unzip vosk-model-small-es-0.42.zip -d models/
rm vosk-model-small-es-0.42.zip

# Windows (PowerShell)
Invoke-WebRequest -Uri "https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip" -OutFile "vosk-model-small-es-0.42.zip"
Expand-Archive -Path "vosk-model-small-es-0.42.zip" -DestinationPath "models\"
Remove-Item "vosk-model-small-es-0.42.zip"
```

## ‚öôÔ∏è Configuraci√≥n

### 1. Configurar variables de entorno

Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
touch .env
```

A√±ade tu API Key de OpenAI:

```bash
OPENAI_API_KEY=tu-api-key-aqui
MLFLOW_TRACKING_URI=http://localhost:8080
MLFLOW_EXPERIMENT_NAME=chatbot-fastapi
```

‚ö†Ô∏è **IMPORTANTE**: Reemplaza `tu-api-key-aqui` con tu API Key real de OpenAI.

### 2. Crear archivos `__init__.py`

El proyecto requiere archivos vac√≠os para que Python reconozca las carpetas como paquetes:

```bash
# macOS/Linux
touch features/__init__.py
touch features/rag_generation/__init__.py
touch features/monitoring/__init__.py
touch routers/__init__.py

# Windows (PowerShell)
New-Item -Path "features\__init__.py" -ItemType File
New-Item -Path "features\rag_generation\__init__.py" -ItemType File
New-Item -Path "features\monitoring\__init__.py" -ItemType File
New-Item -Path "routers\__init__.py" -ItemType File
```

## üéØ Uso

Sigue estos pasos **en orden** para levantar el sistema completo:

### **Paso 1: Levantar Pinecone Local**

Arranca el contenedor Docker de Pinecone:

```bash
docker run -d \
  --name pinecone-local \
  -p 5080:5080 \
  -p 5081:5081 \
  ghcr.io/pinecone-io/pinecone-local:latest
```

Verifica que est√© corriendo:

```bash
curl http://localhost:5080/indexes
# Deber√≠a devolver: {"indexes":[]}
```

Si el contenedor ya existe pero est√° parado:

```bash
docker start pinecone-local
```

### **Paso 2: Indexar documentos en Pinecone**

Ejecuta el script de indexing para cargar los documentos FAQ en la base vectorial:

```bash
python indexing_code.py
```

Deber√≠as ver:

```
Upsert complete
```

### **Paso 3: Levantar MLflow**

En una **nueva terminal** (con el entorno virtual activado):

```bash
mlflow ui --port 8080
```

Accede a MLflow en: http://localhost:8080

### **Paso 4: Levantar el backend**

En otra **nueva terminal** (con el entorno virtual activado):

```bash
uvicorn backend:app --port 8000
```

Espera a ver:

```
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000
```

### **Paso 5: Levantar el frontend**

En otra **nueva terminal** (con el entorno virtual activado):

```bash
streamlit run frontend.py
```

El navegador deber√≠a abrirse autom√°ticamente en: http://localhost:8501

## üéÆ Probando el Sistema

Una vez que todo est√© corriendo:

1. **Chat con texto**: Escribe "¬øQu√© es Python?" en el input del chat
2. **Chat con imagen**: Adjunta una captura de pantalla de c√≥digo PNG
3. **Chat con voz**: Graba un mensaje de voz usando el bot√≥n del micr√≥fono
4. **Monitorizaci√≥n**: Revisa las m√©tricas en MLflow (http://localhost:8080)

## üìÇ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ backend.py                    # Aplicaci√≥n FastAPI
‚îú‚îÄ‚îÄ frontend.py                   # Interfaz Streamlit
‚îú‚îÄ‚îÄ indexing_code.py             # Script de indexing
‚îú‚îÄ‚îÄ faq_pairs.json               # Documentos FAQ
‚îú‚îÄ‚îÄ .env                         # Variables de entorno (NO subir a Git)
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias Python
‚îú‚îÄ‚îÄ recordings/                  # Audios grabados (generado autom√°ticamente)
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chat_with_history.py     # Endpoint /chat_with_history
‚îÇ   ‚îî‚îÄ‚îÄ transcribe.py            # Endpoint /transcribe
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ rag_generation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_generation.py    # Pipeline RAG
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ mlflow_setup.py      # Configuraci√≥n MLflow
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ vosk-model-small-es-0.42/  # Modelo Vosk
```

## üêõ Troubleshooting

### Error: "Connection refused" en puerto 5080

**Causa**: Pinecone Local no est√° corriendo.

**Soluci√≥n**:

```bash
docker start pinecone-local
curl http://localhost:5080/indexes  # Verificar
```

### Error: "Connection refused" en puerto 8080

**Causa**: MLflow no est√° corriendo.

**Soluci√≥n**:

```bash
mlflow ui --port 8080
```

### Error: "Failed to create a model" (Vosk)

**Causa**: El modelo Vosk no est√° descargado o est√° en la ubicaci√≥n incorrecta.

**Soluci√≥n**:

```bash
ls models/vosk-model-small-es-0.42/  # Verificar que existe
# Si no existe, desc√°rgalo siguiendo el paso 4 de Instalaci√≥n
```

### Error: "API request to OpenAI failed"

**Causa**: API Key no configurada o inv√°lida.

**Soluci√≥n**:

1. Verifica que el archivo `.env` existe y contiene tu API Key
2. Aseg√∫rate de que el API Key es v√°lida en https://platform.openai.com/api-keys
3. Reinicia el backend despu√©s de modificar el `.env`

### El backend tarda mucho en arrancar

**Normal**: Vosk tarda 30-60 segundos en cargar el modelo la primera vez. Espera a ver `Application startup complete.`

### Pinecone est√° vac√≠o despu√©s de reiniciar Docker

**Causa**: Pinecone Local pierde datos al reiniciar el contenedor.

**Soluci√≥n**:

```bash
python indexing_code.py  # Re-indexar
```

Para persistir datos entre reinicios, usa un volumen Docker:

```bash
docker run -d \
  --name pinecone-local \
  -p 5080:5080 \
  -p 5081:5081 \
  -v pinecone-data:/data \
  ghcr.io/pinecone-io/pinecone-local:latest
```

## üîí Seguridad

‚ö†Ô∏è **NO subas tu archivo `.env` a Git**. A√±ade `.env` a tu `.gitignore`:

```bash
echo ".env" >> .gitignore
```

‚ö†Ô∏è **NO hardcodees tu API Key** en el c√≥digo. Usa siempre variables de entorno.

## üìù Notas de Desarrollo

- El proyecto usa Pinecone Local para desarrollo sin necesidad de cuenta cloud
- Vosk transcribe localmente, sin enviar audio a servicios externos
- MLflow corre en modo servidor local (no requiere configuraci√≥n adicional)
- El frontend se recarga autom√°ticamente al guardar cambios

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'A√±ade nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para m√°s detalles.

## üë§ Autor

**Miguel S√°nchez Pinto**

- LinkedIn: [tu-perfil](https://www.linkedin.com/in/miguel-s√°nchez-pinto-03771922a)
- GitHub: [@tu-usuario](https://github.com/msp40industry-dev)

## üôè Agradecimientos

- [OpenAI](https://openai.com/) por GPT-4o y embeddings
- [Pinecone](https://www.pinecone.io/) por la base de datos vectorial
- [Vosk](https://alphacephei.com/vosk/) por el modelo de speech-to-text
- [Streamlit](https://streamlit.io/) por el framework de UI
- [MLflow](https://mlflow.org/) por el tracking de experimentos

---

‚≠ê Si este proyecto te fue √∫til, considera darle una estrella en GitHub
